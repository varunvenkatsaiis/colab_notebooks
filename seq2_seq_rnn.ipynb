{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSNJK1q+NWzOr7NF3jXuQ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varunvenkatsaiis/colab_notebooks/blob/main/seq2_seq_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ga3w1y8o_Gi",
        "outputId": "e5c2fa3e-caaa-41ff-d516-a2ce8add35dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-18 14:39:30--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.101.207, 142.250.141.207, 142.251.2.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.101.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip.1’\n",
            "\n",
            "\rspa-eng.zip.1         0%[                    ]       0  --.-KB/s               \rspa-eng.zip.1       100%[===================>]   2.52M  --.-KB/s    in 0.007s  \n",
            "\n",
            "2025-05-18 14:39:30 (346 MB/s) - ‘spa-eng.zip.1’ saved [2638744/2638744]\n",
            "\n",
            "replace spa-eng/_about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace spa-eng/spa.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n"
          ]
        }
      ],
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"spa-eng/spa.txt\""
      ],
      "metadata": {
        "id": "Wr70FbkGp5uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(text_file) as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "\n",
        "for line in lines:\n",
        "  english, spanish = line.split(\"\\t\")\n",
        "  spanish = \"[start] \" + spanish + \" [end]\"\n",
        "  text_pairs.append( (english, spanish) )"
      ],
      "metadata": {
        "id": "94BiEuZlqPFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.shuffle(text_pairs)\n",
        "\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "\n",
        "num_train_samples = len(text_pairs) - 2 * (num_val_samples)\n",
        "\n",
        "train_pairs  = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "\n",
        "test_pairs = text_pairs[ num_train_samples + num_val_samples : ]"
      ],
      "metadata": {
        "id": "pgHIxVbgrcin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import  TextVectorization"
      ],
      "metadata": {
        "id": "L3zBT8Yqsa9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\" ,\"\")\n",
        "strip_chars = strip_chars.replace(\"]\" ,\"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IKQiswZn0Lz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "  lowercase = tf.strings.lower(input_string)\n",
        "  return tf.strings.regex_replace(lowercase , f\"[{re.escape(strip_chars)}]\" , \"\")\n"
      ],
      "metadata": {
        "id": "JlN4ZsHd02cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = sequence_length,\n",
        ")\n",
        "\n",
        "target_vectorization = TextVectorization(\n",
        "    max_tokens = vocab_size,\n",
        "    output_mode = \"int\",\n",
        "    output_sequence_length = sequence_length + 1,\n",
        "    standardize = custom_standardization,\n",
        ")\n",
        "\n",
        "train_english_texts = [ pair[0] for pair in train_pairs ]\n",
        "train_spanish_texts = [ pair[1] for pair in train_pairs ]\n",
        "\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "Qv_n3kwg07s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng , spa):\n",
        "  eng = source_vectorization(eng)\n",
        "  spa = target_vectorization(spa)\n",
        "  return ({\n",
        "      \"english\" : eng,\n",
        "      \"spanish\" : spa[: , :-1],\n",
        "  } , spa[: , 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "  eng_texts , spa_texts = zip(*pairs)\n",
        "  eng_texts = list(eng_texts)\n",
        "  spa_texts = list(spa_texts)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts , spa_texts))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(format_dataset , num_parallel_calls=4)\n",
        "  return dataset.shuffle(2048).prefetch(16).cache()"
      ],
      "metadata": {
        "id": "ovfDzC-C5tux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "test_ds = make_dataset( test_pairs)\n"
      ],
      "metadata": {
        "id": "FuGhG1ru9PyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "source = keras.Input(shape=(None ,) ,dtype=\"int64\" , name = \"english\")\n",
        "x = layers.Embedding(vocab_size ,embed_dim , mask_zero=True)(source)\n",
        "\n",
        "encoded_source = layers.Bidirectional(  layers.GRU(latent_dim) , merge_mode=\"sum\" )(x)"
      ],
      "metadata": {
        "id": "khnTH4om9iAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "past_target = keras.Input( shape=(None , ) , dtype=\"int64\" , name = \"spanish\" )\n",
        "\n",
        "x = layers.Embedding(vocab_size , embed_dim , mask_zero=True ) (past_target)\n",
        "\n",
        "decoder_gru = layers.GRU(latent_dim , return_sequences=True)\n",
        "\n",
        "x = decoder_gru(x , initial_state=encoded_source)\n",
        "\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "target_next_step = layers.Dense(vocab_size , activation=\"softmax\")(x)\n",
        "\n",
        "seq2seq_rnn = keras.Model([source , past_target] , target_next_step)\n",
        "\n"
      ],
      "metadata": {
        "id": "1Hd-SXXlAYov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DM89IhtDPZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}